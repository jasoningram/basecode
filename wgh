from pyspark.sql import Window
from pyspark.sql.functions import col, sum as _sum, lit, min as _min, expr

def weighted_quartiles_by_category(df, catt, numvar, wt):
    # Total weight per category
    total_weight = df.groupBy(catt).agg(_sum(wt).alias("total_wt"))
    df = df.join(total_weight, on=catt)

    # Normalize weight
    df = df.withColumn("norm_wt", col(wt) / col("total_wt"))

    # Cumulative weight ordered by numvar
    w = Window.partitionBy(catt).orderBy(numvar).rowsBetween(Window.unboundedPreceding, 0)
    df = df.withColumn("cum_wt", _sum("norm_wt").over(w))

    # Define thresholds
    thresholds = [0.25, 0.5, 0.75]

    # Initialize empty union DataFrame
    result_df = None
    for q in thresholds:
        # For each category, find the smallest numvar where cum_wt >= quantile
        q_df = (
            df
            .where(col("cum_wt") >= lit(q))
            .groupBy(catt)
            .agg(_min(numvar).alias("value"))
            .withColumn("quantile", lit(q))
        )

        # Union results
        if result_df is None:
            result_df = q_df
        else:
            result_df = result_df.union(q_df)

    return result_df.select(catt, "quantile", "value")

