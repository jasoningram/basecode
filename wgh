from pyspark.sql import Window
from pyspark.sql.functions import col, sum as _sum, lit, when
import numpy as np

def weighted_quartiles_by_category(df, catt, numvar, wt):
    # Normalize weights per group
    window_cat = Window.partitionBy(catt)
    df = df.withColumn("group_weight_sum", _sum(wt).over(window_cat))
    df = df.withColumn("normalized_wt", col(wt) / col("group_weight_sum"))

    # Sort within group
    window_ordered = Window.partitionBy(catt).orderBy(col(numvar))
    df = df.withColumn("cum_wt", _sum("normalized_wt").over(window_ordered))

    # Define the target quantiles
    quantiles = [0.25, 0.5, 0.75]

    # Collect data for interpolation
    pdf = df.select(catt, numvar, "cum_wt").toPandas()

    # Compute weighted quantiles per category
    results = []
    for category, group in pdf.groupby(catt):
        group = group.sort_values(numvar).reset_index(drop=True)
        cum_weights = group["cum_wt"].values
        values = group[numvar].values
        for q in quantiles:
            idx = np.searchsorted(cum_weights, q, side="right")
            if idx == 0:
                quantile_val = values[0]
            elif idx == len(values):
                quantile_val = values[-1]
            else:
                # Linear interpolation
                w1, w2 = cum_weights[idx - 1], cum_weights[idx]
                v1, v2 = values[idx - 1], values[idx]
                quantile_val = v1 + (q - w1) * (v2 - v1) / (w2 - w1)
            results.append((category, q, quantile_val))

    # Convert to Spark DataFrame
    spark = df.sql_ctx.sparkSession
    result_df = spark.createDataFrame(results, [catt, "quantile", "value"])
    return result_df
